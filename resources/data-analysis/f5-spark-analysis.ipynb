{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F5.news Trending News - Machine Learning Exploration\n",
    "\n",
    "- News Article Sentiment\n",
    "- Predict Trending Topics\n",
    "- Topic Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U boto3 hvac mlflow numpy \"pyspark==3.5.0\" python-dotenv \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import hvac\n",
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import Bucketizer, StringIndexer, VectorAssembler, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Vault for Mongo connection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "VaultDown",
     "evalue": "Vault is sealed, on get http://192.168.10.14:8200/v1/auth/token/lookup-self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mVaultDown\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m client \u001b[38;5;241m=\u001b[39m hvac\u001b[38;5;241m.\u001b[39mClient(\n\u001b[1;32m      2\u001b[0m     url\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAULT_ADDR\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m     token\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAULT_TOKEN\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_authenticated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m client\u001b[38;5;241m.\u001b[39mis_authenticated():\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/v1/__init__.py:512\u001b[0m, in \u001b[0;36mClient.is_authenticated\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlookup_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mForbidden:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/v1/__init__.py:433\u001b[0m, in \u001b[0;36mClient.lookup_token\u001b[0;34m(self, token, accessor, wrap_ttl)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    432\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/v1/auth/token/lookup-self\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrap_ttl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrap_ttl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/adapters.py:146\u001b[0m, in \u001b[0;36mAdapter.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Performs a GET request.\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    :param url: Partial URL path to send the request to. This will be joined to the end of the instance's base_uri\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/adapters.py:408\u001b[0m, in \u001b[0;36mJSONAdapter.request\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Main method for routing HTTP requests to the configured Vault base_uri.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    :param args: Positional arguments to pass to RawAdapter.request.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;124;03m    :rtype: dict | requests.Response\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/adapters.py:376\u001b[0m, in \u001b[0;36mRawAdapter.request\u001b[0;34m(self, method, url, headers, raise_exception, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[1;32m    368\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    369\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs\n\u001b[1;32m    373\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok \u001b[38;5;129;01mand\u001b[39;00m (raise_exception \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_exceptions):\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_for_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/adapters.py:294\u001b[0m, in \u001b[0;36mRawAdapter._raise_for_error\u001b[0;34m(self, method, url, response)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    292\u001b[0m     msg \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m--> 294\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/hvac/utils.py:41\u001b[0m, in \u001b[0;36mraise_for_error\u001b[0;34m(method, url, status_code, message, errors, text, json)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_for_error\u001b[39m(\n\u001b[1;32m     16\u001b[0m     method, url, status_code, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m ):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to raise exceptions based on the status code of a response received back from Vault.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    :param method: HTTP method of a request to Vault.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mVaultError\u001b[38;5;241m.\u001b[39mfrom_status(\n\u001b[1;32m     42\u001b[0m         status_code,\n\u001b[1;32m     43\u001b[0m         message,\n\u001b[1;32m     44\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m     45\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m     46\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m     47\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     48\u001b[0m         json\u001b[38;5;241m=\u001b[39mjson,\n\u001b[1;32m     49\u001b[0m     )\n",
      "\u001b[0;31mVaultDown\u001b[0m: Vault is sealed, on get http://192.168.10.14:8200/v1/auth/token/lookup-self"
     ]
    }
   ],
   "source": [
    "client = hvac.Client(\n",
    "    url=os.environ.get('VAULT_ADDR'),\n",
    "    token=os.environ.get('VAULT_TOKEN'),\n",
    ")\n",
    "\n",
    "print(client.is_authenticated())\n",
    "\n",
    "if client.is_authenticated():\n",
    "    try:\n",
    "        secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "            mount_point='kv', \n",
    "            path='f5.news', \n",
    "            raise_on_deleted_version=False\n",
    "        )\n",
    "        \n",
    "        if secret_resp['data'] is not None:\n",
    "            secret_values = secret_resp['data']['data']\n",
    "            for secret, value in secret_values.items():\n",
    "                os.environ[str(secret)] = str(value)\n",
    "        else:\n",
    "            print(\"The secret does not exist.\")\n",
    "    except hvac.exceptions.InvalidPath:\n",
    "        print(\"The path is invalid or the permission is denied.\")\n",
    "    except hvac.exceptions.Forbidden:\n",
    "        print(\"The permission is denied.\")\n",
    "    except hvac.exceptions.VaultError as e:\n",
    "        print(f\"Vault error occurred: {e}\")\n",
    "else:\n",
    "    print(\"Failed to connect to HashiVault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "DEBUG = True\n",
    "MODE = \"local\" # Supported -- local OR cluster\n",
    "REG_PARAM_VALUE = 0.1 # Experimenting with this value can improve final accuracy\n",
    "MAX_ITER = 20\n",
    "DATASET_SPLIT = [0.9, 0.1] # Portion of data to split between training and test datasets\n",
    "SAMPLE_TITLE = \"Trump Says Some Migrants Are ‘Not People’ and Predicts a ‘Blood Bath’ if He Loses\"\n",
    "\n",
    "# Spark\n",
    "SPARK_MASTER = \"spark://localhost:7077\"\n",
    "SPARK_MEMORY = \"4g\"\n",
    "os.environ[\"PYSPARK_PIN_THREAD\"] = \"false\" # TODO: Move to .env\n",
    "\n",
    "# Mongo\n",
    "URI = os.environ['mongo_uri']\n",
    "DATABASE = os.environ['database']\n",
    "COLLECTION = os.environ['collection']\n",
    "\n",
    "# MLflow\n",
    "MLFLOW_API = \"http://localhost:5000\"\n",
    "MODEL_NAME = \"f5news_upvote_bucket_prediction\"\n",
    "EXPERIMENT_NAME = \"f5news_upvote_bucket_prediction\"\n",
    "\n",
    "# Minio S3\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://localhost:9000\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"minio\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"minio123\" # TODO: Move all of these to .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull F5 records using pymongo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # Create a new client and connect to the MongoDB server\n",
    "    client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "\n",
    "    # Send a ping to confirm a successful connection\n",
    "    try:\n",
    "        client.admin.command('ping')\n",
    "        print(\"Successfully connected to MongoDB...\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        database = client[DATABASE]\n",
    "        collection = database[COLLECTION]\n",
    "\n",
    "        # Query all documents in the collection\n",
    "        documents = collection.find({\"sub\": \"politics\"}).sort({\"upvoteCount\": -1, \"fetchedAt\": -1})\n",
    "\n",
    "        if(DEBUG == True):\n",
    "            # Iterate over the cursor to access the documents\n",
    "            for doc in documents:\n",
    "                print(doc[\"title\"])\n",
    "                print(doc[\"fetchedAt\"])\n",
    "                print(doc[\"upvoteCount\"], \"upvotes\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"Mongo documents loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup MLflow runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_run_name = None\n",
    "global_run_id = None\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set MLflow configs\n",
    "mlflow.set_tracking_uri(MLFLOW_API)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "def start_mlflow_run(run_name: str = None):\n",
    "    global global_run_name, global_run_id, start_time\n",
    "    if run_name is None:\n",
    "        run_name = start_time\n",
    "    else:\n",
    "        run_name = run_name + start_time\n",
    "    global_run_name = run_name\n",
    "    run = mlflow.start_run(run_name=run_name, description=EXPERIMENT_NAME)\n",
    "    global_run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow Run Instance\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "start_mlflow_run()\n",
    "\n",
    "# Log parameters\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "mlflow.log_param(\"start_time\", start_time)\n",
    "\n",
    "try:\n",
    "    if MODE == \"local\":\n",
    "        print(\"Starting Spark in local mode\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"F5news\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.mlflow:mlflow-spark:2.8.1\") \\\n",
    "            .getOrCreate()\n",
    "    elif MODE == \"cluster\":\n",
    "        print(\"Starting Spark in cluster mode\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"F5news\") \\\n",
    "            .master(SPARK_MASTER) \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.mlflow:mlflow-spark:2.8.1\") \\\n",
    "            .getOrCreate()\n",
    "    \n",
    "    # Setup Spark AutoLog\n",
    "    mlflow.autolog()\n",
    "\n",
    "    # Get Spark version\n",
    "    spark_version = spark.version\n",
    "    print(\"Spark Version:\", spark_version)\n",
    "\n",
    "    # Check if the master URL indicates local mode or a specific cluster mode\n",
    "    sc = spark.sparkContext\n",
    "    master_url = sc.master\n",
    "    \n",
    "    if \"local\" and not \"localhost\" in master_url:\n",
    "        print(\"PySpark is running in local mode.\")\n",
    "    else:\n",
    "        print(\"PySpark is running in cluster mode with master URL:\", master_url)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))\n",
    "\n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MongoDB as Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Schema\n",
    "schema = StructType([\n",
    "    StructField(\"title\", StringType(), nullable=True), # model input\n",
    "    StructField(\"upvoteCount\", DoubleType(), nullable=True), # used to bucketize for training\n",
    "    StructField(\"fetchedAt\", TimestampType(), nullable=True) # used to filter recent events\n",
    "])\n",
    "\n",
    "# Load data from MongoDB into a DataFrame\n",
    "df = spark.read \\\n",
    "    .format(\"mongo\") \\\n",
    "    .option(\"uri\", URI) \\\n",
    "    .option(\"database\", DATABASE) \\\n",
    "    .option(\"collection\", COLLECTION) \\\n",
    "    .schema(schema) \\\n",
    "    .load()\n",
    "\n",
    "print(\"Data loaded successfully from MongoDB!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Recent Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get document initial count\n",
    "print('Documents Loaded:', df.count())\n",
    "mlflow.log_param(\"loaded_documents\", df.count())\n",
    "\n",
    "# Filter out new posts\n",
    "oneDayAgo = d = datetime.today() - timedelta(days=1)\n",
    "df = df.filter(df.fetchedAt < oneDayAgo)\n",
    "print('Total Filtered Documents:', df.count())\n",
    "\n",
    "mlflow.log_param(\"filtered_documents\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview a Random Sample of Bucketized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    sample_count = 10\n",
    "    pandas_random_sample = df.toPandas().sample(n=sample_count) # Convert to pandas dataframe to take sample\n",
    "    pyspark_random_sample = spark.createDataFrame(pandas_random_sample) # Convert back to pyspark dataframe\n",
    "    pyspark_random_sample.show()\n",
    "    df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizer\n",
    "bucketizer = Bucketizer(splits=[0, 1000, 5000, 10000, 25000, 50000, float('inf')], inputCol=\"upvoteCount\", outputCol=\"bucket\")\n",
    "df = bucketizer.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop unnecessary columns before going into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('upvoteCount')\n",
    "df.drop('fetchedAt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(trainingData, testData) = df.randomSplit(DATASET_SPLIT, seed = 123456)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "mlflow.log_metric(\"trainingData\", trainingData.count())\n",
    "mlflow.log_metric(\"testData\", testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Prep Pipeline Steps\n",
    "\n",
    "- **Regular Expression Tokenizer**: Breaks title into array of words via regex\n",
    "- **Stop Words Remover**: Removes undesireable words from Regex Tokenizer output\n",
    "- **Bag of Words Counter**: Creates vector representation of the array of words extracted from original title string\n",
    "- **Create Label**: Maps all possible values in bucket columns to numeric values (their index position in an array of unique bucket values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# StopWordsRemover\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"reddit\",\"subreddit\"] # TODO: Update stopwords to match dataset\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# CountVectorizer\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Init linear regression model with column names\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"bucket\")\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "    .addGrid(lr.maxIter, [10, 20, 50]) #Number of iterations\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# define evaluator for cross validator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "#cv = CrossValidator(\n",
    "#    estimator=lr, \\\n",
    "#    estimatorParamMaps=paramGrid, \\\n",
    "#    evaluator=evaluator, \\\n",
    "#    numFolds=5\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Data Prep Pipeline\n",
    "\n",
    "Creates the `features` columns. We split titles to words, remove the words we don't want, vectorize the resulting array of words, then label based on bucket column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)\n",
    "dataset = model.transform(trainingData)\n",
    "dataset.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model Using Test Data\n",
    "\n",
    "- **Bucket 1**: 0 - 999 upvotes\n",
    "- **Bucket 2**: 1,000 - 4,999 upvotes\n",
    "- **Bucket 3**: 5,000 - 9,999 upvotes\n",
    "- **Bucket 4**: 10,000 - 24,999 upvotes\n",
    "- **Bucket 5**: 25,000 - 49,999 upvotes\n",
    "- **Bucket 6**: > 50,000 upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions for entire test data set\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# Show a few predictions\n",
    "# - change filter params such as prediction == 1 # TODO: Document what this does\n",
    "if DEBUG:\n",
    "    display(predictions.select(\"title\",\"bucket\",\"probability\",\"prediction\",\"features\").orderBy(\"probability\", ascending=False).toPandas().sample(n=10))\n",
    "\n",
    "# Calculate & Log RMSE\n",
    "rmse = predictions.selectExpr(\"sqrt(avg(pow(bucket - prediction, 2))) as RMSE\").collect()[0][\"RMSE\"]\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Data:\", rmse) # TODO: Determine output label\n",
    "mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# Calculate & Log Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"bucket\")\n",
    "lr_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Logistical Regression Accuracy:\", lr_accuracy)\n",
    "mlflow.log_metric(\"lr_accuracy\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Final Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.spark.log_model(\n",
    "    spark_model = model, \n",
    "    artifact_path = \"model\",\n",
    "#    signature = signature,\n",
    "    registered_model_name = \"f5news_upvote_bucket_prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Final Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    top_level_dir = \"models\"\n",
    "    os.makedirs(top_level_dir, exist_ok=True)\n",
    "\n",
    "    model_dir = os.path.join(top_level_dir, EXPERIMENT_NAME)\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    lr_model.save(os.path.join(model_dir, start_time))\n",
    "except Exception as e:\n",
    "    print(f\"Error saving the model to disk: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.spark.load_model(f'runs:/{global_run_id}/model') # global_run_id is set by when an MLflow run is initiated\n",
    "\n",
    "\n",
    "\n",
    "d = [(\n",
    "    'test'\n",
    ")]\n",
    "modelschema = StructType([\n",
    "    StructField(\"title\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "d2 = [{'title': 'this is a second test'}]\n",
    "#test data\n",
    "test = spark.createDataFrame(d,modelschema)\n",
    "test.show()\n",
    "predictions = model.transform(test)\n",
    "predictions.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Out Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
