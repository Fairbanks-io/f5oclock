{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F5.news Trending News - Machine Learning Exploration\n",
    "\n",
    "- News Article Sentiment\n",
    "- Predict Trending Topics\n",
    "- Topic Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U \"pymongo[srv]\" mlflow pyspark hvac python-dotenv boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import hvac\n",
    "import mlflow\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Vault for Mongo connection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "client = hvac.Client(\n",
    "    url=os.environ.get('VAULT_ADDR'),\n",
    "    token=os.environ.get('VAULT_TOKEN'),\n",
    ")\n",
    "\n",
    "print(client.is_authenticated())\n",
    "\n",
    "if client.is_authenticated():\n",
    "    try:\n",
    "        secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "            mount_point='kv', \n",
    "            path='f5.news', \n",
    "            raise_on_deleted_version=False\n",
    "        )\n",
    "        \n",
    "        if secret_resp['data'] is not None:\n",
    "            secret_values = secret_resp['data']['data']\n",
    "            for secret, value in secret_values.items():\n",
    "                os.environ[str(secret)] = str(value)\n",
    "        else:\n",
    "            print(\"The secret does not exist.\")\n",
    "    except hvac.exceptions.InvalidPath:\n",
    "        print(\"The path is invalid or the permission is denied.\")\n",
    "    except hvac.exceptions.Forbidden:\n",
    "        print(\"The permission is denied.\")\n",
    "    except hvac.exceptions.VaultError as e:\n",
    "        print(f\"Vault error occurred: {e}\")\n",
    "else:\n",
    "    print(\"Failed to connect to HashiVault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "URI = os.environ['mongo_uri']\n",
    "DATABASE = os.environ['database']\n",
    "COLLECTION = os.environ['collection']\n",
    "MLFLOW_API = \"http://localhost:5000\"\n",
    "\n",
    "# Set PYSPARK_PIN_THREAD environment variable to false\n",
    "os.environ[\"PYSPARK_PIN_THREAD\"] = \"false\"\n",
    "\n",
    "REG_PARAM_VALUE = 0.1 # Experimenting with this value can improve final accuracy\n",
    "MAX_ITER = 20\n",
    "DATASET_SPLIT = [0.85, 0.15] # Portion of data to split between training and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull F5 records using pymongo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB...\n",
      "Mongo documents loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new client and connect to the server\n",
    "client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Successfully connected to MongoDB...\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    database = client[DATABASE]\n",
    "    collection = database[COLLECTION]\n",
    "\n",
    "    # Query all documents in the collection\n",
    "    documents = collection.find({\"sub\": \"politics\"}).sort({\"upvoteCount\": -1, \"fetchedAt\": -1})\n",
    "\n",
    "    if(DEBUG == True):\n",
    "        # Iterate over the cursor to access the documents\n",
    "        for doc in documents:\n",
    "            print(doc[\"title\"])\n",
    "            print(doc[\"fetchedAt\"])\n",
    "            print(doc[\"upvoteCount\"], \"upvotes\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Mongo documents loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup MLflow runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_run_name = None\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set MLflow host API\n",
    "mlflow.set_tracking_uri(MLFLOW_API)\n",
    "mlflow.set_experiment(\"f5news_upvote_bucket_prediction\")\n",
    "\n",
    "def start_mlflow_run(run_name: str = None):\n",
    "    global global_run_name, start_time\n",
    "    if run_name is None:\n",
    "        run_name = start_time\n",
    "    else:\n",
    "        run_name = run_name + start_time\n",
    "    global_run_name = run_name\n",
    "    mlflow.start_run(run_name=run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:26:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2024/03/15 02:26:22 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2024/03/15 02:26:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "2024/03/15 02:26:22 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from MongoDB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create MLflow Run Instance\n",
    "start_mlflow_run()\n",
    "mlflow.autolog()\n",
    "\n",
    "# Log parameters\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "mlflow.log_param(\"start_time\", start_time)\n",
    "\n",
    "try:\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"F5news\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.20.0\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Get Spark version\n",
    "    spark_version = spark.version\n",
    "    print(\"Spark version:\", spark_version)\n",
    "\n",
    "    # Load data from MongoDB into a DataFrame\n",
    "    df = spark.read.format(\"mongo\").option(\"uri\", URI).option(\"database\", DATABASE).option(\"collection\", COLLECTION).load()\n",
    "    print(\"Data loaded successfully from MongoDB!\")\n",
    "except Exception as e:\n",
    "    # Error occurred during data loading or model training\n",
    "    print(\"Error:\", str(e))\n",
    "\n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------+--------------+------------+----------------------------------------------------------------------------+-----------+----------------+-----------------------+-------+--------+-----+---------+--------+-------------+----+---------+------------------------------------------------------------------------------------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|__v|_id                       |author        |commentCount|commentLink                                                                 |created_utc|domain          |fetchedAt              |is_self|is_video|media|post_hint|selftext|selftext_html|sub |thumbnail|title                                                                                           |upvoteCount|upvote_ratio|url                                                                                                                                     |\n",
      "+---+--------------------------+--------------+------------+----------------------------------------------------------------------------+-----------+----------------+-----------------------+-------+--------+-----+---------+--------+-------------+----+---------+------------------------------------------------------------------------------------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0  |{65e0f6047e56c7a1e4ab5396}|nirad         |1066        |/r/news/comments/1b37ga3/donald_trump_found_to_have_fraudulently_boosted/   |1709231372 |thenational.scot|2024-03-01 23:44:19.914|false  |false   |NULL |NULL     |        |NULL         |news|         |Donald Trump found to have fraudulently boosted value of Scots homes by up to £200m             |21842      |0.91        |https://www.thenational.scot/news/24143657.donald-trump-found-fraudulently-boosted-value-scots-homes-200m/                              |\n",
      "|0  |{65e112247e56c7a1e4036388}|sue_me_please |170         |/r/news/comments/1b3effu/lgbtq_group_sues_to_block_texas_ag_paxtons/        |1709247994 |keranews.org    |2024-03-01 23:39:19.997|false  |false   |NULL |NULL     |        |NULL         |news|         |LGBTQ group sues to block Texas AG Paxton's request for records about transgender children      |4610       |0.93        |https://www.keranews.org/texas-news/2024-02-29/lgbtq-group-sues-to-block-texas-ag-paxtons-request-for-records-about-transgender-children|\n",
      "|0  |{65e12f707e56c7a1e45d8409}|BarKnight     |411         |/r/news/comments/1b3hbvu/michigan_communities_will_share_a_record_87m_in/   |1709255499 |freep.com       |2024-03-02 00:24:22.398|false  |false   |NULL |NULL     |        |NULL         |news|         |Michigan communities will share a record $87M in marijuana tax revenue                          |6265       |0.97        |https://www.freep.com/story/news/marijuana/2024/02/29/michigan-communities-will-share-a-record-87m-in-marijuana-tax-revenue/72792733007/|\n",
      "|0  |{65e158747e56c7a1e4d232f4}|Silent_killa42|1003        |/r/news/comments/1b3l1u6/irs_launches_crackdown_on_125000_wealthy_nonfilers/|1709266176 |apnews.com      |2024-03-02 14:34:20.029|false  |false   |NULL |NULL     |        |NULL         |news|         |IRS launches crackdown on 125,000 wealthy ‘non-filers’                                          |20920      |0.96        |https://apnews.com/article/irs-tax-season-audit-back-taxes-77c891313f5233366fbe4f6fb5d896e8                                             |\n",
      "|0  |{65e161d47e56c7a1e4ecc798}|jayRIOT       |252         |/r/news/comments/1b3lslv/blockbuster_california_storm_to_deliver_crushing/  |1709268589 |cnn.com         |2024-03-02 00:24:22.398|false  |false   |NULL |NULL     |        |NULL         |news|         |Blockbuster California storm to deliver crushing blow of 10 feet of snow and blizzard conditions|2888       |0.97        |https://www.cnn.com/2024/02/29/weather/california-storm-snow-blizzard-climate/index.html                                                |\n",
      "+---+--------------------------+--------------+------------+----------------------------------------------------------------------------+-----------+----------------+-----------------------+-------+--------+-----+---------+--------+-------------+----+---------+------------------------------------------------------------------------------------------------+-----------+------------+----------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Recent Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Loaded: 4882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:26:37 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:26:37 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Filtered Documents: 4352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:26:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:26:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4352"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document initial count\n",
    "print('Documents Loaded:', df.count())\n",
    "mlflow.log_param(\"loaded_documents\", df.count())\n",
    "\n",
    "# Convert to SQL for familiar data query ability\n",
    "df.createOrReplaceTempView(\"temp\")\n",
    "df = spark.sql(\"SELECT title, upvoteCount, fetchedAt from temp\") \n",
    "\n",
    "# Filter out new posts\n",
    "oneDayAgo = d = datetime.today() - timedelta(days=1)\n",
    "df = df.filter(df.fetchedAt < oneDayAgo)\n",
    "print('Total Filtered Documents:', df.count())\n",
    "\n",
    "mlflow.log_param(\"filtered_documents\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketize by Upvote Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:26:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|     bucket|count|\n",
      "+-----------+-----+\n",
      "|      0-999| 3461|\n",
      "|  1000-4999|  572|\n",
      "|  5000-9999|  188|\n",
      "|10000-24999|  115|\n",
      "|25000-49000|   16|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def upvoteCategorizer(upvotes):\n",
    "    if upvotes < 1000:\n",
    "        return \"0-999\"\n",
    "    if upvotes < 5000:\n",
    "        return \"1000-4999\"\n",
    "    if upvotes < 10000:\n",
    "        return \"5000-9999\"\n",
    "    elif upvotes < 25000:\n",
    "        return \"10000-24999\"\n",
    "    elif upvotes < 50000:\n",
    "        return \"25000-49000\"\n",
    "    else: \n",
    "        return \"50000+\"\n",
    "    \n",
    "bucket_udf = udf(upvoteCategorizer, StringType() )\n",
    "df = df.withColumn(\"bucket\", bucket_udf(\"upvoteCount\"))\n",
    "df.groupBy(\"bucket\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview random sample of bucketized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+---------+\n",
      "|               title|upvoteCount|           fetchedAt|   bucket|\n",
      "+--------------------+-----------+--------------------+---------+\n",
      "|Biden-Bibi clash ...|          0|2024-03-12 22:24:...|    0-999|\n",
      "|Top Haitian gang ...|         28|2024-03-06 07:24:...|    0-999|\n",
      "|r/Politics' 2024 ...|         13|2024-03-06 15:44:...|    0-999|\n",
      "|South Korea begin...|          8|2024-03-11 12:24:...|    0-999|\n",
      "|Polish ex-PM Mora...|         23|2024-03-03 18:34:...|    0-999|\n",
      "|California taxpay...|          4|2024-03-07 06:59:...|    0-999|\n",
      "|March 2024 Nation...|         40|2024-03-07 19:09:...|    0-999|\n",
      "|Anti-Abortion Act...|         46|2024-03-13 05:44:...|    0-999|\n",
      "|He Cursed at a Po...|         63|2024-03-03 18:09:...|    0-999|\n",
      "|Trump Meets TikTo...|       9446|2024-03-10 17:49:...|5000-9999|\n",
      "|Lawmaker who clai...|        165|2024-03-08 01:19:...|    0-999|\n",
      "|Stable Trump Brag...|       2773|2024-03-02 11:19:...|1000-4999|\n",
      "|UN experts condem...|        573|2024-03-06 01:49:...|    0-999|\n",
      "|China's Defence S...|         14|2024-03-05 19:04:...|    0-999|\n",
      "|Chairman Loudermi...|          0|2024-03-12 02:09:...|    0-999|\n",
      "|Woman detained un...|        235|2024-03-02 16:29:...|    0-999|\n",
      "|Indian worker kil...|        160|2024-03-05 15:59:...|    0-999|\n",
      "|Trump's campaign ...|         44|2024-03-12 20:24:...|    0-999|\n",
      "|Russian soldier’s...|         16|2024-03-13 18:04:...|    0-999|\n",
      "|Controversy Surro...|         11|2024-03-12 03:59:...|    0-999|\n",
      "+--------------------+-----------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_count = 20 # TODO: Determine sample size based on loaded data?\n",
    "pandas_random_sample = df.toPandas().sample(n=sample_count) # Convert to pandas dataframe to take sample\n",
    "pyspark_random_sample = spark.createDataFrame(pandas_random_sample) # Convert back to pyspark dataframe\n",
    "pyspark_random_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Prep Pipeline Steps\n",
    "\n",
    "- **Regular Expression Tokenizer**: Breaks title into array of words via regex\n",
    "- **Stop Words Remover**: Removes undesireable words from Regex Tokenizer output\n",
    "- **Bag of Words Counter**: Creates vector representation of the array of words extracted from original title string\n",
    "- **Create Label**: Maps all possible values in bucket columns to numeric values (their index position in an array of unique bucket values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression Tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Stop Words Remover\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # TODO: Update stopwords to match dataset\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Bag of Words Counter\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Create Label\n",
    "label_stringIdx = StringIndexer(inputCol = \"bucket\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Data Prep Pipeline\n",
    "\n",
    "Creates the `features` columns. We split titles to words, remove the words we don't want, vectorize the resulting array of words, then label based on bucket column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:26:58 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: Unable to locate credentials\n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Dataset Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|               title|upvoteCount|           fetchedAt|bucket|               words|            filtered|            features|label|\n",
      "+--------------------+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "|Ultra-conservativ...|         35|2024-03-01 22:09:...| 0-999|[ultra, conservat...|[ultra, conservat...|(1960,[4,39,553,5...|  0.0|\n",
      "|Joe Biden has rai...|         49|2024-03-01 22:14:...| 0-999|[joe, biden, has,...|[joe, biden, has,...|(1960,[2,5,8,11,1...|  0.0|\n",
      "|Oregon takes mass...|          4|2024-03-01 22:14:...| 0-999|[oregon, takes, m...|[oregon, takes, m...|(1960,[407,449,56...|  0.0|\n",
      "|DC Circuit tosses...|         21|2024-03-01 22:19:...| 0-999|[dc, circuit, tos...|[dc, circuit, tos...|(1960,[0,1,3,4,6,...|  0.0|\n",
      "|Shervin Hajipour:...|         34|2024-03-01 22:19:...| 0-999|[shervin, hajipou...|[shervin, hajipou...|(1960,[0,9,10,14,...|  0.0|\n",
      "+--------------------+-----------+--------------------+------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 3704\n",
      "Test Dataset Count: 648\n"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit(DATASET_SPLIT, seed = 123456)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:04 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>' for MLflow schema.\n",
      "2024/03/15 02:27:05 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField('words', ArrayType(StringType(), True), True), StructField('filtered', ArrayType(StringType(), True), True), StructField('features', VectorUDT(), True)]. Model signature is not logged.\n",
      "24/03/15 02:27:05 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2024/03/15 02:27:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during pyspark.ml autologging: Unable to locate credentials\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_param(\"max_iter\", MAX_ITER)\n",
    "mlflow.log_param(\"reg_param_value\", REG_PARAM_VALUE)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=MAX_ITER, regParam=REG_PARAM_VALUE, elasticNetParam=0)\n",
    "lr_model = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model Using Test Data\n",
    "\n",
    "- **Bucket 1**: 0 - 999 upvotes\n",
    "- **Bucket 2**: 1,000 - 4,999 upvotes\n",
    "- **Bucket 3**: 5,000 - 9,999 upvotes\n",
    "- **Bucket 4**: 10,000 - 24,999 upvotes\n",
    "- **Bucket 5**: 25,000 - 49,000 upvotes\n",
    "- **Bucket 6**: > 50,000 upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:27:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-----------+--------------------------------------------------+-----+----------+\n",
      "|                                             title|     bucket|                                       probability|label|prediction|\n",
      "+--------------------------------------------------+-----------+--------------------------------------------------+-----+----------+\n",
      "|Texas Panhandle ranchers face losses and grim t...|  1000-4999|[0.4809630759331583,0.4871018212940306,0.017625...|  1.0|       1.0|\n",
      "|Orbán says Trump won’t give ‘a penny’ to Ukrain...|      0-999|[0.47557323533731616,0.4960344143841745,0.01999...|  0.0|       1.0|\n",
      "|   Trump's RNC makeover signals his plans for 2025|      0-999|[0.4578619695777824,0.47265755244693447,0.04842...|  0.0|       1.0|\n",
      "|Eyebrows raised as Viktor Orbán to visit Donald...|  5000-9999|[0.4500837385929587,0.532439494401897,0.0055974...|  2.0|       1.0|\n",
      "|Zelensky: As long as Ukraine holds, French army...|  5000-9999|[0.44573073111895034,0.4686064371165829,0.05471...|  2.0|       1.0|\n",
      "|Planned Cherokee marijuana superstore in NC ala...|      0-999|[0.44154222833889734,0.46615936447547995,0.0761...|  0.0|       1.0|\n",
      "|Swedish police forcibly remove Greta Thunberg f...|      0-999|[0.4256183873901863,0.5307212762538099,0.025965...|  0.0|       1.0|\n",
      "|Biden Was So Good, Trump Is Accusing Him of Per...|  5000-9999|[0.42286006785313096,0.47453635533360844,0.0250...|  2.0|       1.0|\n",
      "|IDF: Civilians in Rafah will be evacuated to ‘h...|      0-999|[0.4197735257286898,0.42500357057880533,0.12999...|  0.0|       1.0|\n",
      "|Trump just floated Social Security cuts. Take h...|      0-999|[0.41073982412431903,0.4881146475292861,0.05801...|  0.0|       1.0|\n",
      "|Britt backlash stokes GOP fears about losing wo...|      0-999|[0.4091447725853297,0.5036800424247894,0.044630...|  0.0|       1.0|\n",
      "|Biden team brings in $10 million in the 24 hour...|  5000-9999|[0.3726532490544445,0.5738906650463368,0.029239...|  2.0|       1.0|\n",
      "|Husband of Moms for Liberty co-founder won’t fa...|      0-999|[0.3430293505394725,0.44839878566150115,0.03973...|  0.0|       1.0|\n",
      "|Biden may be losing his favorability advantage ...|      0-999|[0.33805635112729554,0.48810228886933865,0.1158...|  0.0|       1.0|\n",
      "|Microscopic plastics could raise risk of stroke...|      0-999|[0.33533335780774787,0.6421370769714868,0.00646...|  0.0|       1.0|\n",
      "|Credit card late fees capped at $8 as part of B...|  1000-4999|[0.33440967628956864,0.6364360789672445,0.01830...|  1.0|       1.0|\n",
      "|The courts were never going to save America fro...|  5000-9999|[0.332625917962425,0.3891044093087254,0.0718832...|  2.0|       1.0|\n",
      "|Blabbering Donald Trump Could Be Sued by E. Jea...|  5000-9999|[0.3324554798426136,0.5775783255768485,0.045495...|  2.0|       1.0|\n",
      "|Biden says he regrets using term ‘illegal,’ as ...|      0-999|[0.3256587962442513,0.6420286516203761,0.023085...|  0.0|       1.0|\n",
      "|Putin allies tell Macron: Any French troops you...|10000-24999|[0.3074485988974407,0.6003111966144162,0.014974...|  3.0|       1.0|\n",
      "+--------------------------------------------------+-----------+--------------------------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Root Mean Squared Error (RMSE) on Test Data: 0.7566576928820131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/15 02:27:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n",
      "2024/03/15 02:27:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during spark autologging: Exception while attempting to initialize JVM-side state for Spark datasource autologging. Note that Spark datasource autologging only works with Spark 3.0 and above. Please create a new Spark session with required Spark version and ensure you have the mlflow-spark JAR attached to your Spark session as described in https://mlflow.org/docs/latest/tracking/autolog.html#spark Exception:\n",
      "'JavaPackage' object is not callable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistical Regression Accuracy: 0.7208857168886239\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions for entire test data set\n",
    "predictions = lr_model.transform(testData)\n",
    "\n",
    "# Show a few predictions\n",
    "# - change filter params such as prediction == 1 # TODO: Document what this does\n",
    "predictions.filter(predictions['prediction'] == 1).select(\"title\",\"bucket\",\"probability\",\"label\",\"prediction\") \\\n",
    ".orderBy(\"probability\", ascending=False).show(n = 20, truncate = 50)\n",
    "\n",
    "# Calculate & Log RMSE\n",
    "rmse = predictions.selectExpr(\"sqrt(avg(pow(label - prediction, 2))) as RMSE\").collect()[0][\"RMSE\"]\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Data:\", rmse) # TODO: Determine output label\n",
    "mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# Calculate & Log Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "lr_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Logistical Regression Accuracy:\", lr_accuracy)\n",
    "mlflow.log_metric(\"lr_accuracy\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Final Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log trained model\n",
    "# mlflow.spark.log_model(lr_model, \"model\") # TODO: Needs further S3 setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Out Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
