{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F5.news Trending News - Machine Learning Exploration\n",
    "\n",
    "- News Article Sentiment\n",
    "- Predict Trending Topics\n",
    "- Topic Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U boto3 hvac mlflow \"pyspark==3.2.4\" python-dotenv \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import hvac\n",
    "import mlflow\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Vault for Mongo connection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "client = hvac.Client(\n",
    "    url=os.environ.get('VAULT_ADDR'),\n",
    "    token=os.environ.get('VAULT_TOKEN'),\n",
    ")\n",
    "\n",
    "print(client.is_authenticated())\n",
    "\n",
    "if client.is_authenticated():\n",
    "    try:\n",
    "        secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "            mount_point='kv', \n",
    "            path='f5.news', \n",
    "            raise_on_deleted_version=False\n",
    "        )\n",
    "        \n",
    "        if secret_resp['data'] is not None:\n",
    "            secret_values = secret_resp['data']['data']\n",
    "            for secret, value in secret_values.items():\n",
    "                os.environ[str(secret)] = str(value)\n",
    "        else:\n",
    "            print(\"The secret does not exist.\")\n",
    "    except hvac.exceptions.InvalidPath:\n",
    "        print(\"The path is invalid or the permission is denied.\")\n",
    "    except hvac.exceptions.Forbidden:\n",
    "        print(\"The permission is denied.\")\n",
    "    except hvac.exceptions.VaultError as e:\n",
    "        print(f\"Vault error occurred: {e}\")\n",
    "else:\n",
    "    print(\"Failed to connect to HashiVault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "DEBUG = False\n",
    "REG_PARAM_VALUE = 0.1 # Experimenting with this value can improve final accuracy\n",
    "MAX_ITER = 20\n",
    "DATASET_SPLIT = [0.85, 0.15] # Portion of data to split between training and test datasets\n",
    "os.environ[\"PYSPARK_PIN_THREAD\"] = \"false\" # TODO: Move to .env\n",
    "\n",
    "# Spark\n",
    "\n",
    "SPARK_MASTER = \"spark://localhost:7077\"\n",
    "SPARK_MEMORY = \"4g\"\n",
    "\n",
    "# Mongo\n",
    "URI = os.environ['mongo_uri']\n",
    "DATABASE = os.environ['database']\n",
    "COLLECTION = os.environ['collection']\n",
    "\n",
    "# MLflow\n",
    "MLFLOW_API = \"http://localhost:5000\"\n",
    "EXPERIMENT_NAME = \"f5news_upvote_bucket_prediction\"\n",
    "\n",
    "# Minio S3\n",
    "os.environ['MLFLOW_S3_ENDPOINT_URL'] = \"http://localhost:9000\"\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"minio\"\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"minio123\" # TODO: Move all of these to .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull F5 records using pymongo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to MongoDB...\n",
      "Mongo documents loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create a new client and connect to the server\n",
    "client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Successfully connected to MongoDB...\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    database = client[DATABASE]\n",
    "    collection = database[COLLECTION]\n",
    "\n",
    "    # Query all documents in the collection\n",
    "    documents = collection.find({\"sub\": \"politics\"}).sort({\"upvoteCount\": -1, \"fetchedAt\": -1})\n",
    "\n",
    "    if(DEBUG == True):\n",
    "        # Iterate over the cursor to access the documents\n",
    "        for doc in documents:\n",
    "            print(doc[\"title\"])\n",
    "            print(doc[\"fetchedAt\"])\n",
    "            print(doc[\"upvoteCount\"], \"upvotes\")\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Mongo documents loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup MLflow runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_run_name = None\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Set MLflow configs\n",
    "mlflow.set_tracking_uri(MLFLOW_API)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "def start_mlflow_run(run_name: str = None):\n",
    "    global global_run_name, start_time\n",
    "    if run_name is None:\n",
    "        run_name = start_time\n",
    "    else:\n",
    "        run_name = run_name + start_time\n",
    "    global_run_name = run_name\n",
    "    mlflow.start_run(run_name=run_name, description=EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/mgmtadmin/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mgmtadmin/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-162f9160-45e1-43a6-9807-a3c7f8ce9671;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "\tfound org.mlflow#mlflow-spark;2.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      ":: resolution report :: resolve 83ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;2.8.1 from central in [default]\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-162f9160-45e1-43a6-9807-a3c7f8ce9671\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/17 04:34:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024/03/17 04:34:37 INFO mlflow.spark.autologging: Autologging successfully enabled for spark.\n",
      "2024/03/17 04:34:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2024/03/17 04:34:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.2.4\n",
      "PySpark is running in local mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully from MongoDB!\n"
     ]
    }
   ],
   "source": [
    "# Create MLflow Run Instance\n",
    "start_mlflow_run()\n",
    "\n",
    "# Log parameters\n",
    "start_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "mlflow.log_param(\"start_time\", start_time)\n",
    "\n",
    "try:\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"F5news\") \\\n",
    "        .master(SPARK_MASTER) \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1,org.mlflow:mlflow-spark:2.8.1\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Setup Spark AutoLog\n",
    "    mlflow.autolog()\n",
    "\n",
    "    # Get Spark version\n",
    "    spark_version = spark.version\n",
    "    print(\"Spark Version:\", spark_version)\n",
    "\n",
    "    # Get the SparkContext from the SparkSession\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Get the master URL from the SparkContext\n",
    "    master_url = sc.master\n",
    "\n",
    "    # Check if the master URL indicates local mode or a specific cluster mode\n",
    "    if \"local\" in master_url:\n",
    "        print(\"PySpark is running in local mode.\")\n",
    "    else:\n",
    "        print(\"PySpark is running in cluster mode with master URL:\", master_url)\n",
    "\n",
    "    # Load data from MongoDB into a DataFrame\n",
    "    df = spark.read.format(\"mongo\").option(\"uri\", URI).option(\"database\", DATABASE).option(\"collection\", COLLECTION).load()\n",
    "    print(\"Data loaded successfully from MongoDB!\")\n",
    "except Exception as e:\n",
    "    # Error occurred during data loading or model training\n",
    "    print(\"Error:\", str(e))\n",
    "\n",
    "    # Stop SparkSession\n",
    "    spark.stop()\n",
    "\n",
    "    # End MLflow run\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Out Recent Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents Loaded: 5520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Filtered Documents: 5205\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5205"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get document initial count\n",
    "print('Documents Loaded:', df.count())\n",
    "mlflow.log_param(\"loaded_documents\", df.count())\n",
    "\n",
    "# Convert to SQL for familiar data query ability\n",
    "df.createOrReplaceTempView(\"temp\")\n",
    "df = spark.sql(\"SELECT title, upvoteCount, fetchedAt from temp\") \n",
    "\n",
    "# Filter out new posts\n",
    "oneDayAgo = d = datetime.today() - timedelta(days=1)\n",
    "df = df.filter(df.fetchedAt < oneDayAgo)\n",
    "print('Total Filtered Documents:', df.count())\n",
    "\n",
    "mlflow.log_param(\"filtered_documents\", df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketize by Upvote Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upvoteCategorizer(upvotes):\n",
    "    if upvotes < 1000:\n",
    "        return \"0-999\"\n",
    "    if upvotes < 5000:\n",
    "        return \"1000-4999\"\n",
    "    if upvotes < 10000:\n",
    "        return \"5000-9999\"\n",
    "    elif upvotes < 25000:\n",
    "        return \"10000-24999\"\n",
    "    elif upvotes < 50000:\n",
    "        return \"25000-49000\"\n",
    "    else: \n",
    "        return \"50000+\"\n",
    "    \n",
    "bucket_udf = udf(upvoteCategorizer, StringType() )\n",
    "df = df.withColumn(\"bucket\", bucket_udf(\"upvoteCount\"))\n",
    "\n",
    "if DEBUG:\n",
    "    df.groupBy(\"bucket\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview a Random Sample of Bucketized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    sample_count = 10\n",
    "    pandas_random_sample = df.toPandas().sample(n=sample_count) # Convert to pandas dataframe to take sample\n",
    "    pyspark_random_sample = spark.createDataFrame(pandas_random_sample) # Convert back to pyspark dataframe\n",
    "    pyspark_random_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Data Prep Pipeline Steps\n",
    "\n",
    "- **Regular Expression Tokenizer**: Breaks title into array of words via regex\n",
    "- **Stop Words Remover**: Removes undesireable words from Regex Tokenizer output\n",
    "- **Bag of Words Counter**: Creates vector representation of the array of words extracted from original title string\n",
    "- **Create Label**: Maps all possible values in bucket columns to numeric values (their index position in an array of unique bucket values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Expression Tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# Stop Words Remover\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] # TODO: Update stopwords to match dataset\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# Bag of Words Counter\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "# Create Label\n",
    "label_stringIdx = StringIndexer(inputCol = \"bucket\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble Data Prep Pipeline\n",
    "\n",
    "Creates the `features` columns. We split titles to words, remove the words we don't want, vectorize the resulting array of words, then label based on bucket column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Data Prep Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/17 04:34:48 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/types/utils.py:393: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
      "24/03/17 04:34:52 WARN StringIndexerModel: Input column bucket does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "2024/03/17 04:34:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\"\n",
      "24/03/17 04:34:52 WARN StringIndexerModel: Input column bucket does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "2024/03/17 04:34:52 WARNING mlflow.pyspark.ml: Model outputs contain unsupported Spark data types: [StructField(words,ArrayType(StringType,true),true), StructField(filtered,ArrayType(StringType,true),true), StructField(features,VectorUDT,true)]. Output schema is not be logged.\n",
      "24/03/17 04:34:52 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2024/03/17 04:34:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpwyms3974/model, flavor: spark). Fall back to return ['pyspark==3.2.4', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Dataset Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Training and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 4444\n",
      "Test Dataset Count: 761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(trainingData, testData) = dataset.randomSplit(DATASET_SPLIT, seed = 123456)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "mlflow.log_metric(\"trainingData\", trainingData.count())\n",
    "mlflow.log_metric(\"testData\", testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Logistic Regression Model (with CrossValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/17 04:36:02 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>' for MLflow schema.\n",
      "24/03/17 04:36:03 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 33) (172.31.0.7 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2878, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/17 04:36:03 WARN TaskSetManager: Lost task 0.1 in stage 45.0 (TID 34) (172.31.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3070, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/17 04:36:03 ERROR TaskSetManager: Task 0 in stage 45.0 failed 4 times; aborting job\n",
      "2024/03/17 04:36:03 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: An error occurred while calling o1064.sumApprox.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 4 times, most recent failure: Lost task 0.3 in stage 45.0 (TID 36) (172.31.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3070, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3070, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/17 04:36:04 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/03/17 04:36:04 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "24/03/17 04:36:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/03/17 04:36:04 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "2024/03/17 04:37:11 WARNING mlflow.pyspark.ml: Model inputs contain unsupported Spark data types: [StructField(words,ArrayType(StringType,true),true), StructField(filtered,ArrayType(StringType,true),true), StructField(features,VectorUDT,true)]. Model signature is not logged.\n",
      "2024/03/17 04:37:13 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmppsbn67h9/model, flavor: spark). Fall back to return ['pyspark==3.2.4', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n",
      "24/03/17 04:38:14 ERROR Instrumentation: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:673)\n",
      "\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.super$save(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$4(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent(events.scala:174)\n",
      "\tat org.apache.spark.ml.MLEvents.withSaveInstanceEvent$(events.scala:169)\n",
      "\tat org.apache.spark.ml.util.Instrumentation.withSaveInstanceEvent(Instrumentation.scala:42)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.$anonfun$save$3$adapted(Pipeline.scala:344)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.PipelineModel$PipelineModelWriter.save(Pipeline.scala:344)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "2024/03/17 04:38:16 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpwqubt7kl/model, flavor: spark). Fall back to return ['pyspark==3.2.4', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "# Init linear regression model with column names\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "    .addGrid(lr.maxIter, [10, 20, 50]) #Number of iterations\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# define evaluator for cross validator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=lr, \\\n",
    "    estimatorParamMaps=paramGrid, \\\n",
    "    evaluator=evaluator, \\\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Train using cross validator and pick the best model\n",
    "lr_model = cv.fit(trainingData) # TODO: How do we extract the params that were chose as the 'best_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model Using Test Data\n",
    "\n",
    "- **Bucket 1**: 0 - 999 upvotes\n",
    "- **Bucket 2**: 1,000 - 4,999 upvotes\n",
    "- **Bucket 3**: 5,000 - 9,999 upvotes\n",
    "- **Bucket 4**: 10,000 - 24,999 upvotes\n",
    "- **Bucket 5**: 25,000 - 49,999 upvotes\n",
    "- **Bucket 6**: > 50,000 upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on Test Data: 0.7586557029970062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/17 04:39:50 WARNING mlflow.data.spark_dataset: Failed to infer schema for Spark dataset. Exception: Unsupported Spark Type '<class 'pyspark.ml.linalg.VectorUDT'>' for MLflow schema.\n",
      "24/03/17 04:39:52 WARN TaskSetManager: Lost task 0.0 in stage 4116.0 (TID 4104) (172.31.0.6 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 3070, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/17 04:39:53 WARN TaskSetManager: Lost task 0.1 in stage 4116.0 (TID 4105) (172.31.0.7 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2878, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/17 04:39:54 ERROR TaskSetManager: Task 0 in stage 4116.0 failed 4 times; aborting job\n",
      "2024/03/17 04:39:54 WARNING mlflow.data.spark_dataset: Encountered an unexpected exception while computing Spark dataset profile. Exception: An error occurred while calling o36422.sumApprox.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4116.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4116.0 (TID 4107) (172.31.0.7 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2878, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 619, in main\n",
      "    process()\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 609, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 2878, in pipeline_func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/pyspark/rdd.py\", line 636, in func\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <lambda>\n",
      "  File \"/home/mgmtadmin/.local/lib/python3.10/site-packages/mlflow/data/spark_dataset.py\", line -1, in <genexpr>\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 138, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 160, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 430, in loads\n",
      "    return pickle.loads(obj, encoding=encoding)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 895, in _parse_datatype_json_string\n",
      "    return _parse_datatype_json_value(json.loads(json_string))\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 912, in _parse_datatype_json_value\n",
      "    return _all_complex_types[tpe].fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in fromJson\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 598, in <listcomp>\n",
      "    return StructType([StructField.fromJson(f) for f in json[\"fields\"]])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 443, in fromJson\n",
      "    _parse_datatype_json_value(json[\"type\"]),\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 914, in _parse_datatype_json_value\n",
      "    return UserDefinedType.fromJson(json_value)\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 757, in fromJson\n",
      "    m = __import__(pyModule, globals(), locals(), [pyClass])\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/__init__.py\", line 22, in <module>\n",
      "    from pyspark.ml.base import Estimator, Model, Predictor, PredictionModel, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 25, in <module>\n",
      "    from pyspark.ml.param.shared import HasInputCol, HasOutputCol, HasLabelCol, HasFeaturesCol, \\\n",
      "  File \"/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/ml/param/__init__.py\", line 21, in <module>\n",
      "    import numpy as np\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.util.StatCounter.merge(StatCounter.scala:55)\n",
      "\tat org.apache.spark.util.StatCounter.<init>(StatCounter.scala:37)\n",
      "\tat org.apache.spark.util.StatCounter$.apply(StatCounter.scala:158)\n",
      "\tat org.apache.spark.rdd.DoubleRDDFunctions.$anonfun$sumApprox$2(DoubleRDDFunctions.scala:110)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistical Regression Accuracy: 0.7152008983922494\n"
     ]
    }
   ],
   "source": [
    "# Make Predictions for entire test data set\n",
    "predictions = lr_model.transform(testData)\n",
    "\n",
    "# Show a few predictions\n",
    "# - change filter params such as prediction == 1 # TODO: Document what this does\n",
    "if DEBUG:\n",
    "    predictions.filter(predictions['prediction'] == 1).select(\"title\",\"bucket\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False).show(n = 10, truncate = 50)\n",
    "\n",
    "# Calculate & Log RMSE\n",
    "rmse = predictions.selectExpr(\"sqrt(avg(pow(label - prediction, 2))) as RMSE\").collect()[0][\"RMSE\"]\n",
    "print(\"Root Mean Squared Error (RMSE) on Test Data:\", rmse) # TODO: Determine output label\n",
    "mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# Calculate & Log Accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "lr_accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Logistical Regression Accuracy:\", lr_accuracy)\n",
    "mlflow.log_metric(\"lr_accuracy\", lr_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Final Model to MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/03/17 04:39:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmplkevt9b8/model, flavor: spark). Fall back to return ['pyspark==3.2.4', 'pandas<2']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    }
   ],
   "source": [
    "# Log trained model\n",
    "mlflow.spark.log_model(lr_model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Final Model to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_level_dir = \"models\"\n",
    "os.makedirs(top_level_dir, exist_ok=True)\n",
    "\n",
    "model_dir = os.path.join(top_level_dir, EXPERIMENT_NAME)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "lr_model.save(os.path.join(model_dir, start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Out Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
