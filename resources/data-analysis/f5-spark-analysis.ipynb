{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F5.news Trending News - Machine Learning Exploration\n",
    "\n",
    "- News Article Sentiment\n",
    "- Predict Trending Topics\n",
    "- Topic Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U \"pymongo[srv]\" mlflow pyspark hvac python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hvac\n",
    "import mlflow\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Vault for Mongo connection values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = hvac.Client(\n",
    "    url=os.environ.get('VAULT_ADDR'),\n",
    "    token=os.environ.get('VAULT_TOKEN'),\n",
    ")\n",
    "\n",
    "print(client.is_authenticated())\n",
    "\n",
    "if client.is_authenticated():\n",
    "    try:\n",
    "        secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "            mount_point='kv', \n",
    "            path='f5.news', \n",
    "            raise_on_deleted_version=False\n",
    "        )\n",
    "        \n",
    "        if secret_resp['data'] is not None:\n",
    "            secret_values = secret_resp['data']['data']\n",
    "            for secret, value in secret_values.items():\n",
    "                os.environ[str(secret)] = str(value)\n",
    "        else:\n",
    "            print(\"The secret does not exist.\")\n",
    "    except hvac.exceptions.InvalidPath:\n",
    "        print(\"The path is invalid or the permission is denied.\")\n",
    "    except hvac.exceptions.Forbidden:\n",
    "        print(\"The permission is denied.\")\n",
    "    except hvac.exceptions.VaultError as e:\n",
    "        print(f\"Vault error occurred: {e}\")\n",
    "else:\n",
    "    print(\"Failed to connect to HashiVault\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URI = os.environ['mongo_uri']\n",
    "DATABASE = os.environ['database']\n",
    "COLLECTION = os.environ['collection']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull F5 records using pymongo client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new client and connect to the server\n",
    "client = MongoClient(URI, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Successfully connected to MongoDB\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "try:\n",
    "    database = client[DATABASE]\n",
    "    collection = database[COLLECTION]\n",
    "\n",
    "    # Query all documents in the collection\n",
    "    documents = collection.find({\"sub\": \"politics\"}).sort({\"upvoteCount\": -1, \"fetchedAt\": -1})\n",
    "\n",
    "    # Iterate over the cursor to access the documents\n",
    "    for doc in documents:\n",
    "        print(doc[\"title\"])\n",
    "        print(doc[\"fetchedAt\"])\n",
    "        print(doc[\"upvoteCount\"], \"upvotes\")\n",
    "        print()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"param1\", \"value1\")\n",
    "    mlflow.log_param(\"param2\", \"value2\")\n",
    "    \n",
    "    try:\n",
    "         # Create a SparkSession\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"F5news\") \\\n",
    "            .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        # Load data from MongoDB into a DataFrame\n",
    "        df = spark.read.format(\"mongo\").option(\"uri\", URI).option(\"database\", DATABASE).option(\"collection\", COLLECTION).load()\n",
    "        print(\"Data loaded successfully from MongoDB!\")\n",
    "        \n",
    "        # Show loaded data\n",
    "        df.printSchema()\n",
    "        print('Total Count:', df.count())\n",
    "        df.show(1,truncate=False)\n",
    "    except Exception as e:\n",
    "        # Error occurred during data loading or model training\n",
    "        print(\"Error:\", str(e))\n",
    "\n",
    "        # Stop SparkSession\n",
    "        spark.stop()\n",
    "\n",
    "        # End MLflow run\n",
    "        mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful because there are type issues when using the pyspark dataframe issue directly (maybe because of schema? or null objects? or maybe sometimes a column has an int and sometimes a string, idk..)\n",
    "df.createOrReplaceTempView(\"temp\")\n",
    "df = spark.sql(\"SELECT title, upvoteCount, fetchedAt from temp\") \n",
    "\n",
    "\n",
    "# filter out new posts\n",
    "timeAgo = d = datetime.today() - timedelta(days=1)\n",
    "df = df.filter(df.fetchedAt < timeAgo)\n",
    "print('count after date filter', df.count())\n",
    "\n",
    "\n",
    "# Bucket by upvote count\n",
    "def upvoteCategorizer(upvotes):\n",
    "    if upvotes < 1000:\n",
    "        return \"0-999\"\n",
    "    if upvotes < 5000:\n",
    "        return \"1000-4999\"\n",
    "    if upvotes < 10000:\n",
    "        return \"5000-9999\"\n",
    "    elif upvotes < 25000:\n",
    "        return \"10000-24999\"\n",
    "    elif upvotes < 50000:\n",
    "        return \"25000-49000\"\n",
    "    else: \n",
    "        return \"50000+\"\n",
    "    \n",
    "from pyspark.sql.functions import udf\n",
    "bucket_udf = udf(upvoteCategorizer, StringType() )\n",
    "df = df.withColumn(\"bucket\", bucket_udf(\"upvoteCount\"))\n",
    "from pyspark.sql.functions import col\n",
    "df.groupBy(\"bucket\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# select sample dataset\n",
    "sample_count = 20\n",
    "pandas_random_sample = df.toPandas().sample(n=sample_count) # convert to pandas dataframe to take sample\n",
    "pyspark_random_sample = spark.createDataFrame(pandas_random_sample) # convert back to pyspark dataframe\n",
    "pyspark_random_sample.show()\n",
    "\n",
    "\n",
    "# tokenize and remove stop words\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"title\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "\n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=30000, minDF=5)\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "label_stringIdx = StringIndexer(inputCol = \"bucket\", outputCol = \"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(df)\n",
    "dataset = pipelineFit.transform(df)\n",
    "dataset.show(25)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(trainingData)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "predictions = lr_model.transform(testData)\n",
    "rmse = predictions.selectExpr(\"sqrt(avg(pow(upvoteCount - prediction, 2))) as RMSE\").collect()[0][\"RMSE\"]\n",
    "print(\"Root Mean Squared Error (RMSE) on test data:\", rmse)\n",
    "\n",
    "# Log metrics\n",
    "mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "# Log trained model\n",
    "mlflow.spark.log_model(lr_model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkSession\n",
    "spark.stop()\n",
    "\n",
    "# End MLflow run\n",
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
